"""
WebChatBot - åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
æ”¯æŒå¤šè§’è‰²å¯¹è¯ç®¡ç†ã€åŠ¨æ€è®°å¿†æœºåˆ¶å’ŒRAGå¢å¼º
"""
import os
import json
import time
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils.conversation import Conversation, Message
from utils.memory import SimpleMemory, DynamicMemory, SummaryMemory
from rag.retriever import DocumentRetriever
from rag.knowledge_base import KnowledgeBase

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åº”ç”¨æ ‡é¢˜
st.set_page_config(
    page_title="WebChatBot - æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
if not os.path.exists("data"):
    os.makedirs("data")
if not os.path.exists("data/conversations"):
    os.makedirs("data/conversations")
if not os.path.exists("data/vector_db"):
    os.makedirs("data/vector_db")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    """åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€"""
    if "conversation" not in st.session_state:
        # é»˜è®¤ç³»ç»Ÿæç¤º
        default_system_message = """ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚ä½ å¯ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæä¾›ä¿¡æ¯ï¼Œå¹¶ååŠ©å®Œæˆå„ç§ä»»åŠ¡ã€‚
è¯·ä¿æŒå›ç­”ç®€æ´ã€å‡†ç¡®å’Œæœ‰å¸®åŠ©ã€‚å¦‚æœä½ ä¸çŸ¥é“æŸä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´å‡ºæ¥ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚"""
        
        # åˆ›å»ºå¯¹è¯å®ä¾‹
        st.session_state.conversation = Conversation(
            system_message=default_system_message,
            memory=DynamicMemory()  # é»˜è®¤ä½¿ç”¨åŠ¨æ€è®°å¿†
        )
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹é…ç½®
    if "embedding_provider" not in st.session_state:
        st.session_state.embedding_provider = os.getenv("EMBEDDING_PROVIDER", "openai")
    
    if "embedding_model" not in st.session_state:
        st.session_state.embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-ada-002")
    
    if "use_local_embeddings" not in st.session_state:
        st.session_state.use_local_embeddings = os.getenv("USE_LOCAL_EMBEDDINGS", "false").lower() == "true"
    
    if "local_embedding_model" not in st.session_state:
        st.session_state.local_embedding_model = os.getenv(
            "LOCAL_EMBEDDING_MODEL", 
            "sentence-transformers/all-MiniLM-L6-v2"
        )
    
    if "retriever" not in st.session_state:
        # åˆ›å»ºçŸ¥è¯†åº“å®ä¾‹
        knowledge_base = KnowledgeBase(
            embedding_model=st.session_state.embedding_model,
            use_local_embeddings=st.session_state.use_local_embeddings,
            local_embedding_model=st.session_state.local_embedding_model,
            embedding_provider=st.session_state.embedding_provider
        )
        
        # åˆ›å»ºæ£€ç´¢å™¨å®ä¾‹
        st.session_state.retriever = DocumentRetriever(knowledge_base=knowledge_base)
    
    if "rag_enabled" not in st.session_state:
        st.session_state.rag_enabled = False
    
    if "memory_type" not in st.session_state:
        st.session_state.memory_type = "dynamic"
    
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

# åˆå§‹åŒ–ä¼šè¯
init_session_state()

# ä¾§è¾¹æ è®¾ç½®
st.sidebar.title("WebChatBot è®¾ç½®")

# æ¨¡å‹é€‰æ‹©
model_options = {
    "gpt-3.5-turbo": "GPT-3.5 Turbo",
    "gpt-4": "GPT-4",
    "gpt-4-turbo": "GPT-4 Turbo",
}

# æ ¹æ®ç¯å¢ƒå˜é‡æ·»åŠ å¯ç”¨çš„æ¨¡å‹
if os.getenv("ZHIPU_API_KEY"):
    model_options.update({
        "glm-4": "æ–‡å¿ƒä¸€è¨€ GLM-4",
        "glm-3-turbo": "æ–‡å¿ƒä¸€è¨€ GLM-3"
    })
    logger.info("å·²å¯ç”¨æ–‡å¿ƒä¸€è¨€æ¨¡å‹æ”¯æŒ")

if os.getenv("DEEPSEEK_API_KEY"):
    model_options.update({
        "deepseek-chat": "DeepSeek Chat",
        "deepseek-coder": "DeepSeek Coder"
    })
    logger.info("å·²å¯ç”¨DeepSeekæ¨¡å‹æ”¯æŒ")

if os.getenv("SPARK_APP_ID") and os.getenv("SPARK_API_KEY") and os.getenv("SPARK_API_SECRET"):
    model_options.update({
        "spark-v3": "è®¯é£æ˜Ÿç« V3",
        "spark-v2": "è®¯é£æ˜Ÿç« V2",
        "spark-v1.5": "è®¯é£æ˜Ÿç« V1.5"
    })
    logger.info("å·²å¯ç”¨è®¯é£æ˜Ÿç«æ¨¡å‹æ”¯æŒ")

if os.getenv("VOLCENGINE_API_KEY"):
    model_options.update({
        "volc-v3": "ç«å±±å¼•æ“ V3",
        "volc-v2": "ç«å±±å¼•æ“ V2"
    })
    logger.info("å·²å¯ç”¨ç«å±±å¼•æ“æ¨¡å‹æ”¯æŒ")

if os.getenv("GOOGLE_API_KEY"):
    model_options.update({
        "gemini-pro": "Google Gemini Pro",
        "gemini-pro-vision": "Google Gemini Pro Vision"
    })
    logger.info("å·²å¯ç”¨Google Geminiæ¨¡å‹æ”¯æŒ")

# æ·»åŠ Ollamaæ¨¡å‹é€‰é¡¹
if os.getenv("OLLAMA_BASE_URL"):
    ollama_models = ["llama2", "mistral", "gemma"]
    for model in ollama_models:
        model_options.update({
            f"ollama/{model}": f"Ollama {model.capitalize()}"
        })
    logger.info("å·²å¯ç”¨Ollamaæ¨¡å‹æ”¯æŒ")

selected_model = st.sidebar.selectbox(
    "é€‰æ‹©æ¨¡å‹",
    options=list(model_options.keys()),
    format_func=lambda x: model_options[x],
    index=0
)

# æ¸©åº¦å‚æ•°
temperature = st.sidebar.slider(
    "æ¸©åº¦ (åˆ›é€ æ€§)",
    min_value=0.0,
    max_value=1.0,
    value=0.7,
    step=0.1,
    help="è¾ƒä½çš„å€¼ä½¿å›ç­”æ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿å›ç­”æ›´å¤šæ ·åŒ–å’Œåˆ›é€ æ€§"
)

# è®°å¿†æœºåˆ¶é€‰æ‹©
memory_type = st.sidebar.radio(
    "è®°å¿†æœºåˆ¶",
    options=["simple", "dynamic", "summary"],
    format_func=lambda x: {
        "simple": "ç®€å•è®°å¿† (å›ºå®šæ¶ˆæ¯æ•°)",
        "dynamic": "åŠ¨æ€è®°å¿† (æ™ºèƒ½é€‰æ‹©é‡è¦æ¶ˆæ¯)",
        "summary": "æ‘˜è¦è®°å¿† (é•¿å¯¹è¯è‡ªåŠ¨æ‘˜è¦)"
    }[x],
    index=1,
    help="é€‰æ‹©ä¸åŒçš„è®°å¿†æœºåˆ¶æ¥ç®¡ç†å¯¹è¯å†å²"
)

# å¦‚æœè®°å¿†ç±»å‹æ”¹å˜ï¼Œæ›´æ–°å¯¹è¯çš„è®°å¿†æœºåˆ¶
if memory_type != st.session_state.memory_type:
    if memory_type == "simple":
        st.session_state.conversation.memory = SimpleMemory(max_messages=10)
    elif memory_type == "dynamic":
        st.session_state.conversation.memory = DynamicMemory()
    elif memory_type == "summary":
        st.session_state.conversation.memory = SummaryMemory()
    
    st.session_state.memory_type = memory_type

# RAGè®¾ç½®
with st.sidebar.expander("RAGè®¾ç½®", expanded=False):
    rag_enabled = st.checkbox(
        "å¯ç”¨RAGå¢å¼º",
        value=st.session_state.rag_enabled,
        help="å¯ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºå›ç­”"
    )
    
    # åµŒå…¥æ¨¡å‹æä¾›å•†é€‰æ‹©
    embedding_providers = {
        "openai": "OpenAI",
        "deepseek": "DeepSeek",
        "local": "æœ¬åœ°æ¨¡å‹"
    }
    
    # è·å–å½“å‰åµŒå…¥æä¾›å•†
    current_provider = "openai"
    if "embedding_provider" in st.session_state:
        current_provider = st.session_state.embedding_provider
    
    embedding_provider = st.selectbox(
        "åµŒå…¥æ¨¡å‹æä¾›å•†",
        options=list(embedding_providers.keys()),
        format_func=lambda x: embedding_providers[x],
        index=list(embedding_providers.keys()).index(current_provider),
        help="é€‰æ‹©ç”¨äºæ–‡æ¡£åµŒå…¥çš„æ¨¡å‹æä¾›å•†"
    )
    
    # æ ¹æ®æä¾›å•†æ˜¾ç¤ºä¸åŒçš„æ¨¡å‹é€‰é¡¹
    if embedding_provider == "openai":
        openai_models = ["text-embedding-ada-002", "text-embedding-3-small", "text-embedding-3-large"]
        
        # è·å–å½“å‰æ¨¡å‹
        current_model = "text-embedding-ada-002"
        if "embedding_model" in st.session_state and st.session_state.embedding_model in openai_models:
            current_model = st.session_state.embedding_model
        
        embedding_model = st.selectbox(
            "OpenAIåµŒå…¥æ¨¡å‹",
            options=openai_models,
            index=openai_models.index(current_model),
            help="é€‰æ‹©OpenAIåµŒå…¥æ¨¡å‹"
        )
    
    elif embedding_provider == "deepseek":
        deepseek_models = ["deepseek-embeddings", "deepseek-embeddings-v1"]
        
        # è·å–å½“å‰æ¨¡å‹
        current_model = "deepseek-embeddings"
        if "embedding_model" in st.session_state and st.session_state.embedding_model in deepseek_models:
            current_model = st.session_state.embedding_model
        
        embedding_model = st.selectbox(
            "DeepSeekåµŒå…¥æ¨¡å‹",
            options=deepseek_models,
            index=deepseek_models.index(current_model) if current_model in deepseek_models else 0,
            help="é€‰æ‹©DeepSeekåµŒå…¥æ¨¡å‹"
        )
    
    elif embedding_provider == "local":
        local_models = [
            "sentence-transformers/all-MiniLM-L6-v2",
            "sentence-transformers/all-mpnet-base-v2",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ]
        
        # è·å–å½“å‰æ¨¡å‹
        current_model = "sentence-transformers/all-MiniLM-L6-v2"
        if "local_embedding_model" in st.session_state:
            current_model = st.session_state.local_embedding_model
        
        embedding_model = st.selectbox(
            "æœ¬åœ°åµŒå…¥æ¨¡å‹",
            options=local_models,
            index=local_models.index(current_model) if current_model in local_models else 0,
            help="é€‰æ‹©æœ¬åœ°Hugging FaceåµŒå…¥æ¨¡å‹"
        )
        
        st.info("æœ¬åœ°åµŒå…¥æ¨¡å‹éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œé¦–æ¬¡ä½¿ç”¨å¯èƒ½è¾ƒæ…¢ã€‚")
    
    # åº”ç”¨RAGè®¾ç½®æŒ‰é’®
    if st.button("åº”ç”¨RAGè®¾ç½®"):
        # ä¿å­˜è®¾ç½®åˆ°ä¼šè¯çŠ¶æ€
        st.session_state.rag_enabled = rag_enabled
        st.session_state.embedding_provider = embedding_provider
        
        if embedding_provider == "local":
            st.session_state.local_embedding_model = embedding_model
            st.session_state.use_local_embeddings = True
        else:
            st.session_state.embedding_model = embedding_model
            st.session_state.use_local_embeddings = False
        
        # é‡æ–°åˆå§‹åŒ–æ£€ç´¢å™¨
        with st.spinner("æ­£åœ¨åº”ç”¨RAGè®¾ç½®..."):
            # åˆ›å»ºæ–°çš„çŸ¥è¯†åº“å®ä¾‹
            knowledge_base = KnowledgeBase(
                embedding_model=st.session_state.get("embedding_model"),
                use_local_embeddings=st.session_state.get("use_local_embeddings", False),
                local_embedding_model=st.session_state.get("local_embedding_model"),
                embedding_provider=st.session_state.get("embedding_provider")
            )
            
            # åˆ›å»ºæ–°çš„æ£€ç´¢å™¨å®ä¾‹
            st.session_state.retriever = DocumentRetriever(knowledge_base=knowledge_base)
            
            # æ›´æ–°å¯¹è¯çš„RAGè®¾ç½®
            if rag_enabled:
                st.session_state.conversation.rag_retriever = st.session_state.retriever
            else:
                st.session_state.conversation.rag_retriever = None
            
            st.success("RAGè®¾ç½®å·²åº”ç”¨")

# å¦‚æœRAGè®¾ç½®æ”¹å˜ä½†æ²¡æœ‰ç‚¹å‡»åº”ç”¨æŒ‰é’®ï¼Œæ˜¾ç¤ºæç¤º
if "last_rag_enabled" not in st.session_state:
    st.session_state.last_rag_enabled = rag_enabled

if rag_enabled != st.session_state.last_rag_enabled:
    st.sidebar.info("RAGè®¾ç½®å·²æ›´æ”¹ï¼Œè¯·ç‚¹å‡»'åº”ç”¨RAGè®¾ç½®'æŒ‰é’®ä½¿æ›´æ”¹ç”Ÿæ•ˆã€‚")
    st.session_state.last_rag_enabled = rag_enabled

# çŸ¥è¯†åº“ç®¡ç†
with st.sidebar.expander("çŸ¥è¯†åº“ç®¡ç†", expanded=False):
    # æ˜¾ç¤ºå½“å‰çŸ¥è¯†åº“çŠ¶æ€
    doc_count = st.session_state.retriever.get_document_count()
    st.write(f"å½“å‰çŸ¥è¯†åº“ä¸­æœ‰ {doc_count} ä¸ªæ–‡æ¡£å—")
    
    # æ˜¾ç¤ºå½“å‰åµŒå…¥æ¨¡å‹ä¿¡æ¯
    if st.session_state.use_local_embeddings:
        st.info(f"å½“å‰ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹: {st.session_state.local_embedding_model}")
    else:
        provider = st.session_state.embedding_provider.capitalize()
        model = st.session_state.embedding_model
        st.info(f"å½“å‰ä½¿ç”¨{provider}åµŒå…¥æ¨¡å‹: {model}")
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“",
        type=["txt", "pdf", "csv", "md", "html"],
        help="æ”¯æŒTXT, PDF, CSV, Markdownå’ŒHTMLæ ¼å¼"
    )
    
    if uploaded_file is not None:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        file_path = f"data/uploads/{uploaded_file.name}"
        os.makedirs("data/uploads", exist_ok=True)
        
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
            chunk_count = st.session_state.retriever.add_document(file_path)
            if chunk_count > 0:
                st.success(f"å·²æ·»åŠ  {chunk_count} ä¸ªæ–‡æ¡£å—åˆ°çŸ¥è¯†åº“")
            else:
                st.error("æ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–åµŒå…¥æ¨¡å‹é…ç½®")
    
    # æ–‡æœ¬è¾“å…¥
    text_input = st.text_area("æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬æ·»åŠ åˆ°çŸ¥è¯†åº“")
    if st.button("æ·»åŠ æ–‡æœ¬"):
        if text_input:
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡æœ¬..."):
                chunk_count = st.session_state.retriever.add_text(
                    text_input,
                    metadata={"source": "ç”¨æˆ·è¾“å…¥", "timestamp": datetime.now().isoformat()}
                )
                if chunk_count > 0:
                    st.success(f"å·²æ·»åŠ  {chunk_count} ä¸ªæ–‡æ¡£å—åˆ°çŸ¥è¯†åº“")
                else:
                    st.error("æ–‡æœ¬å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥åµŒå…¥æ¨¡å‹é…ç½®")
    
    # é‡å»ºçŸ¥è¯†åº“
    if st.button("é‡å»ºçŸ¥è¯†åº“", help="ä½¿ç”¨å½“å‰åµŒå…¥æ¨¡å‹è®¾ç½®é‡æ–°æ„å»ºçŸ¥è¯†åº“"):
        with st.spinner("æ­£åœ¨é‡å»ºçŸ¥è¯†åº“..."):
            # åˆ›å»ºæ–°çš„çŸ¥è¯†åº“å®ä¾‹
            knowledge_base = KnowledgeBase(
                embedding_model=st.session_state.embedding_model,
                use_local_embeddings=st.session_state.use_local_embeddings,
                local_embedding_model=st.session_state.local_embedding_model,
                embedding_provider=st.session_state.embedding_provider
            )
            
            # åˆ›å»ºæ–°çš„æ£€ç´¢å™¨å®ä¾‹
            st.session_state.retriever = DocumentRetriever(knowledge_base=knowledge_base)
            
            # æ›´æ–°å¯¹è¯çš„RAGè®¾ç½®
            if st.session_state.rag_enabled:
                st.session_state.conversation.rag_retriever = st.session_state.retriever
            
            st.success("çŸ¥è¯†åº“å·²é‡å»º")
    
    # æ¸…ç©ºçŸ¥è¯†åº“
    if st.button("æ¸…ç©ºçŸ¥è¯†åº“", type="primary", use_container_width=True):
        with st.spinner("æ­£åœ¨æ¸…ç©ºçŸ¥è¯†åº“..."):
            st.session_state.retriever.clear_knowledge_base()
            st.success("çŸ¥è¯†åº“å·²æ¸…ç©º")

# ç³»ç»Ÿæç¤ºè®¾ç½®
with st.sidebar.expander("ç³»ç»Ÿæç¤ºè®¾ç½®", expanded=False):
    # è·å–å½“å‰ç³»ç»Ÿæç¤º
    current_system_message = next(
        (msg.content for msg in st.session_state.conversation.messages if msg.role == "system"),
        "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"
    )
    
    # ç¼–è¾‘ç³»ç»Ÿæç¤º
    new_system_message = st.text_area(
        "ç¼–è¾‘ç³»ç»Ÿæç¤º",
        value=current_system_message,
        height=150
    )
    
    if st.button("æ›´æ–°ç³»ç»Ÿæç¤º"):
        # æ›´æ–°ç³»ç»Ÿæç¤º
        # é¦–å…ˆåˆ é™¤æ‰€æœ‰ç³»ç»Ÿæ¶ˆæ¯
        st.session_state.conversation.messages = [
            msg for msg in st.session_state.conversation.messages if msg.role != "system"
        ]
        # æ·»åŠ æ–°çš„ç³»ç»Ÿæ¶ˆæ¯
        st.session_state.conversation.add_message("system", new_system_message)
        st.success("ç³»ç»Ÿæç¤ºå·²æ›´æ–°")

# å¯¹è¯å†å²ç®¡ç†
with st.sidebar.expander("å¯¹è¯å†å²ç®¡ç†", expanded=False):
    # ä¿å­˜å½“å‰å¯¹è¯
    if st.button("ä¿å­˜å½“å‰å¯¹è¯"):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = f"data/conversations/conversation_{timestamp}.json"
        
        st.session_state.conversation.save_conversation(file_path)
        st.success(f"å¯¹è¯å·²ä¿å­˜åˆ°: {file_path}")
    
    # åˆ—å‡ºä¿å­˜çš„å¯¹è¯
    saved_conversations = []
    if os.path.exists("data/conversations"):
        saved_conversations = [f for f in os.listdir("data/conversations") if f.endswith(".json")]
    
    if saved_conversations:
        selected_conversation = st.selectbox(
            "åŠ è½½ä¿å­˜çš„å¯¹è¯",
            options=saved_conversations,
            format_func=lambda x: x.replace("conversation_", "").replace(".json", "")
        )
        
        if st.button("åŠ è½½é€‰å®šçš„å¯¹è¯"):
            file_path = f"data/conversations/{selected_conversation}"
            st.session_state.conversation.load_conversation(file_path)
            # æ›´æ–°èŠå¤©å†å²
            st.session_state.chat_history = [
                {"role": msg.role, "content": msg.content}
                for msg in st.session_state.conversation.get_history(include_system=False)
            ]
            st.success("å¯¹è¯å·²åŠ è½½")
    
    # æ¸…é™¤å½“å‰å¯¹è¯
    if st.button("æ¸…é™¤å½“å‰å¯¹è¯", type="primary"):
        st.session_state.conversation.clear_history()
        st.session_state.chat_history = []
        st.success("å¯¹è¯å·²æ¸…é™¤")

# ä¸»ç•Œé¢
st.title("WebChatBot - æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ")
st.markdown("""
è¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼Œæ”¯æŒå¤šè§’è‰²å¯¹è¯ç®¡ç†ã€åŠ¨æ€è®°å¿†æœºåˆ¶å’ŒRAGå¢å¼ºã€‚
""")

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.chat_history:
    if message["role"] == "user":
        st.chat_message("user").write(message["content"])
    elif message["role"] == "assistant":
        st.chat_message("assistant").write(message["content"])

# ç”¨æˆ·è¾“å…¥
user_input = st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜...")

# å¤„ç†ç”¨æˆ·è¾“å…¥
if user_input:
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.chat_message("user").write(user_input)
    
    # æ·»åŠ åˆ°å†å²
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    
    # æ›´æ–°æ¨¡å‹è®¾ç½®
    st.session_state.conversation.model_name = selected_model
    st.session_state.conversation.temperature = temperature
    
    # ç”Ÿæˆå›å¤
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            # ç”Ÿæˆå›å¤
            response = st.session_state.conversation.generate_response(user_input)
            
            # æ˜¾ç¤ºå›å¤
            st.write(response)
            
            # æ·»åŠ åˆ°å†å²
            st.session_state.chat_history.append({"role": "assistant", "content": response})

# æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
if os.getenv("DEBUG", "false").lower() == "true":
    with st.expander("è°ƒè¯•ä¿¡æ¯", expanded=False):
        st.write("å½“å‰ä¼šè¯çŠ¶æ€:")
        st.json({
            "memory_type": st.session_state.memory_type,
            "rag_enabled": st.session_state.rag_enabled,
            "model": selected_model,
            "temperature": temperature,
            "message_count": len(st.session_state.conversation.messages)
        })